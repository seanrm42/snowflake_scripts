use role accountadmin;
create or replace warehouse admin_wh;
use schema snowflake.account_usage;

/*-----------------------------------------------------------------------------------------
Data Ingest with Snowpipe and "Copy" (Tier1)
Description:
This query returns an aggregated daily summary of all loads for each table in Snowflake 
showing average file size, total rows, total volume and the ingest method (copy or snowpipe)
How to Interpret Results:
With this high-level information you can determine if file sizes are too small or too big 
for optimal ingest. If you can map the volume to credit consumption you can determine which 
tables are consuming more credits per TB loaded.
*/-----------------------------------------------------------------------------------------

SELECT QUERY_ID
,USER_NAME
,WAREHOUSE_NAME
,WAREHOUSE_SIZE
,BYTES_SCANNED
,BYTES_SPILLED_TO_REMOTE_STORAGE
,BYTES_SPILLED_TO_REMOTE_STORAGE / BYTES_SCANNED AS SPILLING_READ_RATIO
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
WHERE BYTES_SPILLED_TO_REMOTE_STORAGE > BYTES_SCANNED * 5  -- Each byte read was spilled 5x on average
ORDER BY SPILLING_READ_RATIO DESC;

/*-----------------------------------------------------------------------------------------
Scale Up vs. Out (Size vs. Multi-cluster) (Tier2)
Description:
Two separate queries that list out the warehouses and times that could benefit from either a 
MCW setting OR scaling up to a larger size.
How to Interpret Results:
Use this list to determine reconfiguration of a warehouse and the times or users that are 
causing contention on the warehouse
*/-----------------------------------------------------------------------------------------

--LIST OF WAREHOUSES AND DAYS WHERE MCW COULD HAVE HELPED
SELECT TO_DATE(START_TIME) as DATE
,WAREHOUSE_NAME
,SUM(AVG_RUNNING) AS SUM_RUNNING
,SUM(AVG_QUEUED_LOAD) AS SUM_QUEUED
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."WAREHOUSE_LOAD_HISTORY"
WHERE start_time::date > dateadd('days', -90, current_date)
GROUP BY 1,2
HAVING SUM(AVG_QUEUED_LOAD) >0
;

--LIST OF WAREHOUSES AND QUERIES WHERE A LARGER WAREHOUSE WOULD HAVE HELPED WITH REMOTE SPILLING
SELECT QUERY_ID
,USER_NAME
,WAREHOUSE_NAME
,WAREHOUSE_SIZE
,BYTES_SCANNED
,BYTES_SPILLED_TO_REMOTE_STORAGE
,BYTES_SPILLED_TO_REMOTE_STORAGE / BYTES_SCANNED AS SPILLING_READ_RATIO
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
WHERE BYTES_SPILLED_TO_REMOTE_STORAGE > BYTES_SCANNED * 5  -- Each byte read was spilled 5x on average
ORDER BY SPILLING_READ_RATIO DESC
;

/*-----------------------------------------------------------------------------------------
Warehouse Cache Usage (Tier3)
Description:
Aggregate across all queries broken out by warehouses showing the percentage of data scanned 
from the warehouse cache.
How to Interpret Results:
Look for warehouses that are used from querying/reporting and have a low percentage. This 
indicates that the warehouse is suspending too quickly
*/-----------------------------------------------------------------------------------------

SELECT WAREHOUSE_NAME
,COUNT(*) AS QUERY_COUNT
,SUM(BYTES_SCANNED) AS BYTES_SCANNED
,SUM(BYTES_SCANNED*PERCENTAGE_SCANNED_FROM_CACHE) AS BYTES_SCANNED_FROM_CACHE
,SUM(BYTES_SCANNED*PERCENTAGE_SCANNED_FROM_CACHE) / SUM(BYTES_SCANNED) AS PERCENT_SCANNED_FROM_CACHE
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
WHERE start_time::date > dateadd('days', -90, current_date)
AND BYTES_SCANNED > 0
GROUP BY 1
ORDER BY 5
;

/*-----------------------------------------------------------------------------------------
Heavy Scanners (Tier3)
Description:
Ordered list of users that run queries that scan a lot of data.
How to Interpret Results:
This is a potential opportunity to train the user or enable clustering.
*/-----------------------------------------------------------------------------------------

select 
  User_name
, warehouse_name
, avg(case when partitions_total > 0 then partitions_scanned / partitions_total else 0 end) avg_pct_scanned
from   snowflake.account_usage.query_history
where  start_time::date > dateadd('days', -90, current_date)
group by 1, 2
order by 3 desc
;

/*-----------------------------------------------------------------------------------------
Full Table Scans by User (Tier3)
Description:
These queries are the list of users that run the most queries with near full table scans 
and then the list of the queries themselves.
How to Interpret Results:
This is a potential opportunity to train the user or enable clustering.
*/-----------------------------------------------------------------------------------------

--who are the users with the most (near) full table scans
SELECT USER_NAME
,COUNT(*) as COUNT_OF_QUERIES
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
WHERE start_time::date > dateadd('days', -90, current_date)
AND PARTITIONS_SCANNED > (PARTITIONS_TOTAL*0.95)
AND QUERY_TYPE NOT LIKE 'CREATE%'
group by 1
order by 2 desc;

-- This gives all queries in the last month with nearly a full table scan :) > 95%, ordered by the worst offending
SELECT * 
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
WHERE start_time::date > dateadd('days', -90, current_date)
AND PARTITIONS_SCANNED > (PARTITIONS_TOTAL*0.95)
AND QUERY_TYPE NOT LIKE 'CREATE%'
ORDER BY PARTITIONS_SCANNED DESC
LIMIT 50  -- Configurable threshold that defines "TOP N=50"
;

/*-----------------------------------------------------------------------------------------
Top 10 Spillers Remote (Tier3)
Description:
Identifies the top 10 worst offending queries in terms of bytes spilled to remote storage.
How to Interpret Results:
These queries should most likely be run on larger warehouses that have more local storage 
and memory.
*/-----------------------------------------------------------------------------------------

select query_id, substr(query_text, 1, 50) partial_query_text, user_name, warehouse_name, warehouse_size, 
       BYTES_SPILLED_TO_REMOTE_STORAGE, start_time, end_time, total_elapsed_time/1000 total_elapsed_time
from   snowflake.account_usage.query_history
where  BYTES_SPILLED_TO_REMOTE_STORAGE > 0
and start_time::date > dateadd('days', -90, current_date)
order  by BYTES_SPILLED_TO_REMOTE_STORAGE desc
limit 10
;

/*-----------------------------------------------------------------------------------------
AutoClustering History & 7-Day Average (Tier3)
Description:
Average daily credits consumed by Auto-Clustering grouped by week over the last year.
How to Interpret Results:
Look for anomalies in the daily average over the course of the year. Opportunity to 
investigate the spikes or changes in consumption.
*/-----------------------------------------------------------------------------------------

WITH CREDITS_BY_DAY AS (
SELECT TO_DATE(START_TIME) as DATE
,SUM(CREDITS_USED) as CREDITS_USED
FROM "SNOWFLAKE"."ACCOUNT_USAGE"."AUTOMATIC_CLUSTERING_HISTORY"
WHERE START_TIME >= dateadd(year,-1,current_timestamp()) 
GROUP BY 1
ORDER BY 2 DESC 
  )
SELECT DATE_TRUNC('week',DATE)
,AVG(CREDITS_USED) as AVG_DAILY_CREDITS
FROM CREDITS_BY_DAY
GROUP BY 1
ORDER BY 1
;

/*-----------------------------------------------------------------------------------------
Materialized Views History & 7-Day Average (Tier3)
Description:
Average daily credits consumed by Materialized Views grouped by week over the last year.
How to Interpret Results:
Look for anomalies in the daily average over the course of the year. Opportunity to 
investigate the spikes or changes in consumption.
*/-----------------------------------------------------------------------------------------

WITH CREDITS_BY_DAY AS (
SELECT TO_DATE(START_TIME) as DATE
,SUM(CREDITS_USED) as CREDITS_USED


FROM "SNOWFLAKE"."ACCOUNT_USAGE"."MATERIALIZED_VIEW_REFRESH_HISTORY"

WHERE START_TIME >= dateadd(year,-1,current_timestamp()) 
GROUP BY 1
ORDER BY 2 DESC 
  )
  
SELECT DATE_TRUNC('week',DATE)
,AVG(CREDITS_USED) as AVG_DAILY_CREDITS
FROM CREDITS_BY_DAY
GROUP BY 1
ORDER BY 1
;

/*-----------------------------------------------------------------------------------------
Search Optimization History & 7-Day Average (Tier3)
Description:
Average daily credits consumed by Search Optimization grouped by week over the last year.
How to Interpret Results:
Look for anomalies in the daily average over the course of the year. Opportunity to 
investigate the spikes or changes in consumption.
*/-----------------------------------------------------------------------------------------

WITH CREDITS_BY_DAY AS (
SELECT TO_DATE(START_TIME) as DATE
,SUM(CREDITS_USED) as CREDITS_USED


FROM "SNOWFLAKE"."ACCOUNT_USAGE"."SEARCH_OPTIMIZATION_HISTORY"

WHERE START_TIME >= dateadd(year,-1,current_timestamp()) 
GROUP BY 1
ORDER BY 2 DESC 
  )
  
SELECT DATE_TRUNC('week',DATE)
,AVG(CREDITS_USED) as AVG_DAILY_CREDITS
FROM CREDITS_BY_DAY
GROUP BY 1
ORDER BY 1
;

/*-----------------------------------------------------------------------------------------
Snowpipe History & 7-Day Average (Tier3)
Description:
Average daily credits consumed by Snowpipe grouped by week over the last year.
How to Interpret Results:
Look for anomalies in the daily average over the course of the year. Opportunity to 
investigate the spikes or changes in consumption.
*/-----------------------------------------------------------------------------------------

WITH CREDITS_BY_DAY AS (
SELECT TO_DATE(START_TIME) as DATE
,SUM(CREDITS_USED) as CREDITS_USED


FROM "SNOWFLAKE"."ACCOUNT_USAGE"."PIPE_USAGE_HISTORY"

WHERE START_TIME >= dateadd(year,-1,current_timestamp()) 
GROUP BY 1
ORDER BY 2 DESC 
  )
  
SELECT DATE_TRUNC('week',DATE)
,AVG(CREDITS_USED) as AVG_DAILY_CREDITS
FROM CREDITS_BY_DAY
GROUP BY 1
ORDER BY 1
;

/*-----------------------------------------------------------------------------------------
Replication History & 7-Day Average (Tier3)
Description:
Average daily credits consumed by Replication grouped by week over the last year.
How to Interpret Results:
Look for anomalies in the daily average over the course of the year. Opportunity to 
investigate the spikes or changes in consumption.
*/-----------------------------------------------------------------------------------------

WITH CREDITS_BY_DAY AS (
SELECT TO_DATE(START_TIME) as DATE
,SUM(CREDITS_USED) as CREDITS_USED


FROM "SNOWFLAKE"."ACCOUNT_USAGE"."REPLICATION_USAGE_HISTORY"

WHERE START_TIME >= dateadd(year,-1,current_timestamp()) 
GROUP BY 1
ORDER BY 2 DESC 
  )
  
SELECT DATE_TRUNC('week',DATE)
,AVG(CREDITS_USED) as AVG_DAILY_CREDITS
FROM CREDITS_BY_DAY
GROUP BY 1
ORDER BY 1
;
